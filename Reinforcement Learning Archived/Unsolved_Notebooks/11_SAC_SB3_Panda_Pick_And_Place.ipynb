{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713488284646,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"eqw1ngsM2ybh"},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":83288,"status":"ok","timestamp":1713488395587,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"1ssM7RSv28Tz"},"outputs":[],"source":["%pip install gymnasium stable-baselines3 panda_gym\n","\n","clear_output()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":618,"status":"ok","timestamp":1713488961756,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"bN_EtEkz3m-X"},"outputs":[],"source":["# %pip install numpy matplotlib"]},{"cell_type":"markdown","metadata":{"id":"v_9qidbm3pnb"},"source":["# Content\n","\n","In this notebook, you will use the Soft Actor Critic algorithm to solve the **Panda Pick and Place** Environment. We will use Stable baselines3's implementation of SAC. The Panda pick and place environment is found in panda-gym, which provides some robotics environments in pybullet simulation as gym environments.\n","\n","- Write code to define and train the agent\n","- Also include a visualization of the agent's performance in the form of a video\n","\n","In the Panda Pick and Place environment, a robotic arm needs to learn to pick an object on the table and place it on the goal position (near another object in this environment). The actions control the robotic arm. Here is a biref detail of the actions and rewards of the environment:\n","\n","| **Aspect**               | **Description**                                                                                                                                                           |\n","|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n","| **Actions**              |                                                                                                                                                                           |\n","| Cartesian End-Effector Movements | Controls the position and orientation of the robot's end-effector (e.g., x, y, z coordinates and roll, pitch, yaw).                                                        |\n","| Joint Movements          | Directly controls the individual joint angles of the robot arm by specifying the target positions for each joint.                                                          |\n","| Gripper Control          | Controls the opening and closing of the gripper, which is crucial for grasping and releasing objects.                                                                      |\n","| Discrete or Continuous Actions | The action space can be either discrete (specific predefined movements) or continuous (exact positions or velocities).                                                         |\n","| **Rewards**              |                                                                                                                                                                           |\n","| Distance to Target       | Rewards (or penalties) based on the distance between the object and the target location. As the object gets closer to the target, the reward increases.                    |\n","| Grasp Success            | Rewards for successfully grasping the object. Typically a binary reward (e.g., +1 for a successful grasp, 0 otherwise).                                                    |\n","| Placement Success        | Rewards for successfully placing the object at the target location, often a significant reward indicating task completion.                                                 |\n","| Intermediate Rewards     | Rewards for intermediate steps such as moving the end-effector close to the object, aligning the gripper, or lifting the object off the table.                             |\n","| Penalties                | Penalties for undesirable actions such as dropping the object, moving away from the target, or colliding with the environment.                                             |\n","\n","\n","![PandaPickAndPlace Image](https://github.com/qgallouedec/panda-gym/raw/master/docs/_static/img/pickandplace.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12579,"status":"ok","timestamp":1713488959980,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"-xPuTSjF2mBM"},"outputs":[],"source":["import numpy as np\n","\n","import gymnasium as gym\n","import panda_gym\n","\n","# Stable baseline 3 importd\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","# import remaining imports here\n","\n","from IPython.display import clear_output\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from IPython.display import HTML"]},{"cell_type":"markdown","metadata":{"id":"YefuGTt35sJW"},"source":["## Creating the environment"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":418,"status":"ok","timestamp":1713488984457,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Q3c38k562mBP"},"outputs":[],"source":["def make_env(render_mode=\"rgb_array\"):\n","\n","    env = gym.make('PandaPickAndPlace-v3', render_mode=render_mode)\n","    return env"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1736,"status":"ok","timestamp":1713488991949,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"sX3Q4-ZU2mBQ"},"outputs":[],"source":["# Create the environment\n","env = DummyVecEnv([make_env for _ in range(4)])  # adjust accoring to available ram"]},{"cell_type":"markdown","metadata":{"id":"s_kIiUmF6I-x"},"source":["### Solve here\n","\n","write the code to define and train the agent:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mqW1KvMa6WQV"},"source":["### Visualization\n","\n","You are provided with some functions which will help you visualize the results as a video.\n","Feel free to wrie your own code for visualization if you prefer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiN7qBgM2mBS"},"outputs":[],"source":["def frames_to_video(frames, fps=24):\n","    # Do not modify\n","\n","    fig = plt.figure(figsize=(frames[0].shape[1] / 100, frames[0].shape[0] / 100), dpi=100)\n","    ax = plt.axes()\n","    ax.set_axis_off()\n","\n","    if len(frames[0].shape) == 2:  # Grayscale image\n","        im = ax.imshow(frames[0], cmap='gray')\n","    else:  # Color image\n","        im = ax.imshow(frames[0])\n","\n","    def init():\n","        if len(frames[0].shape) == 2:\n","            im.set_data(frames[0], cmap='gray')\n","        else:\n","            im.set_data(frames[0])\n","        return im,\n","\n","    def update(frame):\n","        if len(frames[frame].shape) == 2:\n","            im.set_data(frames[frame], cmap='gray')\n","        else:\n","            im.set_data(frames[frame])\n","        return im,\n","\n","    interval = 1000 / fps\n","    anim = FuncAnimation(fig, update, frames=len(frames), init_func=init, blit=True, interval=interval)\n","    plt.close()\n","    return HTML(anim.to_html5_video())"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":14661,"status":"ok","timestamp":1713489208022,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"2nBggG0z2mBS"},"outputs":[],"source":["t_env = DummyVecEnv([lambda: make_env(render_mode=\"rgb_array\")])\n","state = t_env.reset()\n","frames = []\n","\n","while True:\n","    \n","    # Write your code to choose an action here.\n","    action = 0\n","\n","\n","    \n","    state_next, r, done, info = t_env.step(action)\n","    frames.append(t_env.render())\n","    state = state_next\n","    if done:\n","        break\n","\n","t_env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ldPDsif2mBS","outputId":"2cf8099e-106f-460b-d2d9-9a113a218b58"},"outputs":[],"source":["frames_to_video(frames, fps=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vO6WTjq2mBS"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"kaustenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
