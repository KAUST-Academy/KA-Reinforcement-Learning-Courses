{"cells":[{"cell_type":"markdown","id":"57f2f350-4c97-4700-9707-184e3cedd16f","metadata":{"id":"57f2f350-4c97-4700-9707-184e3cedd16f"},"source":["###  Introduction to Reinforcement Learning (RL) and Markov Decision Process (MDP)\n","\n","## Objectives:\n","\n","In this notebook, your objectives are to create environments using OpenAI-gym\n","and then write and evaluate simple policies(no RL training) on the created environments\n","\n","The environments you will work with are \"Cart Pole\" and \"Frozen Lake\""]},{"cell_type":"markdown","id":"8ce1582e-d13d-410f-a6d1-dea7df636b59","metadata":{"id":"8ce1582e-d13d-410f-a6d1-dea7df636b59"},"source":["### Definitions\n","\n","Reinforcement Learning (RL) is a branch of machine learning that focuses on learning optimal actions to take in a given environment to maximize a cumulative reward or return.\n","\n","Markov Decision Process (MDP) is a mathematical framework for modeling decision-making problems in RL. It consists of states, actions, transition probabilities, and rewards.\n"]},{"cell_type":"markdown","id":"faf75453-1877-495b-b1dd-16ba52cc0dbe","metadata":{"id":"faf75453-1877-495b-b1dd-16ba52cc0dbe"},"source":["### OpenAI Gym"]},{"cell_type":"markdown","id":"fecb42a8-324d-4bd4-b3b8-e60b7c911d50","metadata":{"id":"fecb42a8-324d-4bd4-b3b8-e60b7c911d50"},"source":["[OpenAI Gym](https://www.gymlibrary.dev/) is a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results. OpenAI Gym provides a diverse suite of environments that range from easy to difficult and involve many different kinds of data.\n","\n","Creating and Interacting with gym environments is very simple.\n","\n","```\n","import gym\n","env = gym.make(\"CartPole-v1\")\n","observation, info = env.reset(seed=42)\n","\n","for _ in range(1000):\n","    action = env.action_space.sample()\n","    observation, reward, done, truncated, info = env.step(action)\n","\n","    if terminated or truncated:\n","        observation, info = env.reset()\n","env.close()\n","```\n","\n","Following are the definitions of some common terminologies used.\n","\n","**Reset:** Resets the environment to an initial state and returns the initial observation. <br>\n","**Step:** Run one timestep of the environment's dynamics.<br>\n","**Observation:** The observed state of the environment.<br>\n","**Action:** An action provided by the agent.<br>\n","**Reward:** The amount of reward returned as a result of taking the action.<br>\n","**Terminated:** Whether a terminal state (as defined under the MDP of the task) is reached.<br>\n","**Truncated:** Whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent physically going out of bounds.<br>\n","**Info:** This contains auxiliary diagnostic information (helpful for debugging, learning, and logging).<br>\n","**Action Space:** This attribute gives the format of valid actions. It is of datatype Space provided by Gym. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.<br>\n","**Observation:** This attribute gives the format of valid observations. It is of datatype Space provided by Gym. For example, if the observation space is of type Box and the shape of the object is (4,), this denotes a valid observation will be an array of 4 numbers.<br>\n","\n","Note: Previously, `terminated` and `truncated` used to be merged under one variable `done`. <br>\n","\n","\n","We will use OpenAI Gym for Frozen Lake (2D) and Cart Pole (1D) environments.\n","\n","### Note:\n","\n","OpenAI has launched the Gymnasium library. Gymnasium is the successor to OpenAI Gym, designed to maintain and extend the original framework for reinforcement learning environments. It offers improved support, more active development, and enhanced features while maintaining compatibility with existing Gym-based code. The interface for gymnasium and gym is almost same.\n","\n","Gym itself still exists and is used, and we will stick to it in this notebook as well"]},{"cell_type":"markdown","id":"fcdc8ec7-b351-4ca9-9d7f-831c31111ee6","metadata":{"id":"fcdc8ec7-b351-4ca9-9d7f-831c31111ee6"},"source":["# Cart Pole\n","\n","|   |   |\n","|---|---|\n","| Action Space | Discrete(2) |\n","| Observation Shape | (4,) |\n","| Observation High | [4.8   inf 0.42  inf] |\n","| Observation Low | [-4.8   -inf -0.42  -inf] |\n","| Import | `gym.make(\"CartPole-v1\")` |\n","\n","\n","### Description\n","\n","This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n","[\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n","A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n","The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n"," in the left and right direction on the cart.\n","\n","\n","### Action Space\n","\n","The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n"," of the fixed force the cart is pushed with.\n","\n","| Num | Action                 |\n","|-----|------------------------|\n","| 0   | Push cart to the left  |\n","| 1   | Push cart to the right |\n","\n","\n","**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n"," the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n","\n","### Observation Space\n","\n","The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n","\n","| Num | Observation           | Min                 | Max               |\n","|-----|-----------------------|---------------------|-------------------|\n","| 0   | Cart Position         | -4.8                | 4.8               |\n","| 1   | Cart Velocity         | -Inf                | Inf               |\n","| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n","| 3   | Pole Angular Velocity | -Inf                | Inf               |\n","\n","**Note:** While the ranges above denote the possible values for observation space of each element,\n","    it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n","-  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n","   if the cart leaves the `(-2.4, 2.4)` range.\n","-  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n","   if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n","\n","### Rewards\n","\n","Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n","including the termination step, is allotted. The threshold for rewards is 475 for v1.\n","\n","### Starting State\n","\n","All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n","\n","### Episode End\n","\n","The episode ends if any one of the following occurs:\n","\n","1. Termination: Pole Angle is greater than ±12°\n","2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n","3. Truncation: Episode length is greater than 500 (200 for v0)"]},{"cell_type":"markdown","id":"67aebdb6-239c-4555-a8a3-d28e499a5d27","metadata":{"id":"67aebdb6-239c-4555-a8a3-d28e499a5d27"},"source":["Docs/source: https://www.gymlibrary.dev/environments/classic_control/cart_pole/"]},{"cell_type":"markdown","id":"f7188440-8fb8-4ecb-85c9-77fc797ec6f7","metadata":{"id":"f7188440-8fb8-4ecb-85c9-77fc797ec6f7"},"source":["# Policies\n","A policy is a mapping from states to actions. It determines the action to take in each state.\n","\n","In the following code cells, you will define and evaluate different policies for the 1D and 2D environments.\n"]},{"cell_type":"code","execution_count":17,"id":"5ff64163-5a64-41c8-9141-c7bd2eb5e231","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1717935660496,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"5ff64163-5a64-41c8-9141-c7bd2eb5e231","outputId":"0aea6ea2-e7d9-4889-d0e3-a6f810a5f5d7","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import gym\n","from tqdm import tqdm\n","\n","import numpy as np\n","\n","# For visualization\n","from gym.wrappers.monitoring import video_recorder\n","from IPython.display import HTML\n","from IPython import display\n","import glob\n","import base64, io, os\n","\n","os.environ['SDL_VIDEODRIVER']='dummy'"]},{"cell_type":"code","execution_count":18,"id":"ed784bc8-1f05-4907-9044-9e17cebb2484","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717935660497,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"ed784bc8-1f05-4907-9044-9e17cebb2484","tags":[]},"outputs":[],"source":["# Function to evaluate a policy in an environment\n","def evaluate_policy(env, policy, num_episodes=1000):\n","    rewards = []\n","    for _ in tqdm(range(num_episodes)):\n","        state = env.reset()\n","        episode_reward = 0\n","        done = False\n","        while not done:\n","            action = policy(state)\n","            next_state, reward, done, info = env.step(action)\n","            episode_reward += reward\n","            state = next_state\n","        rewards.append(episode_reward)\n","    return sum(rewards) / num_episodes"]},{"cell_type":"markdown","id":"cd6291cb-4fa5-46cc-92db-aa4ebc9de0e3","metadata":{"id":"cd6291cb-4fa5-46cc-92db-aa4ebc9de0e3"},"source":["## 1D"]},{"cell_type":"markdown","id":"fc105317","metadata":{},"source":["Create the CartPole Environment\n","\n","Maybe try looking at it's observation space and action space"]},{"cell_type":"code","execution_count":null,"id":"673a9de1","metadata":{},"outputs":[],"source":["env_1d = None"]},{"cell_type":"markdown","id":"07f8456d","metadata":{},"source":["### Define CartPole Policies\n","\n","Implement all the functions below to create the specify policy.\n","\n","Feel free to define any extra policies you can think of."]},{"cell_type":"code","execution_count":15,"id":"a0b213db-5909-4961-a7f5-8985da6ac40f","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717935647678,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"a0b213db-5909-4961-a7f5-8985da6ac40f","tags":[]},"outputs":[],"source":["# Write an angle policy: which decides the action based on the angle of the pole\n","\n","def angle_policy(observation):\n","    # The observation here is gym's observation. Look at the action space or review the docs to\n","    # see how it works.\n","    pass\n","\n","# Define the \"Always Left\" policy\n","def always_left_policy(observation):\n","    pass\n","\n","# Define the \"Always Right\" policy\n","def always_right_policy(observation):\n","    pass\n","\n","# Define a random policy\n","def random_policy(observation):\n","    pass"]},{"cell_type":"markdown","id":"8136f46d","metadata":{},"source":["### Cartpole policy evaluations\n","\n","evaluate the policies you've made above"]},{"cell_type":"code","execution_count":null,"id":"dcc26b41-cf9d-4e8b-a950-e9326defa445","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96726,"status":"ok","timestamp":1717936112889,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"dcc26b41-cf9d-4e8b-a950-e9326defa445","outputId":"7223f066-a01e-481f-eaa3-1fc584df2510","tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0d5a0543-ec5c-41c1-a821-0c908b8b5488","metadata":{"id":"0d5a0543-ec5c-41c1-a821-0c908b8b5488"},"source":["### Visualization\n","\n","You are provided with the helper code to visualize the policy on environments.\n","Feel free to write your own visualization code if you want\n","\n","Visualize atleast one policy - the best one, for each environment and clearly label which policy you are visualizing\n","\n","You can visualize more than one policies if you want but make sure you include the best policy"]},{"cell_type":"code","execution_count":27,"id":"3aea3f97-6e6c-4b84-b6a4-29d376313b8e","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717936240672,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"3aea3f97-6e6c-4b84-b6a4-29d376313b8e","tags":[]},"outputs":[],"source":["# Helper functions for visualization of policies on the environment.\n","# Don't modify the code in this cell\n","\n","os.makedirs(\"video\", exist_ok=True)\n","\n","def show_video(env_name):\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = 'video/{}.mp4'.format(env_name)\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","def show_video_of_model(env_name, env, policy, max_steps=10000, verbose=0):\n","    vid = video_recorder.VideoRecorder(env, path=\"video/{}.mp4\".format(env_name))\n","    state = env.reset()\n","    done = False\n","    for t in tqdm(range(max_steps)):\n","        vid.capture_frame()\n","        action = policy(state)\n","        next_state, reward, done, info = env.step(action)\n","        if verbose:\n","            print(f\"state: {state}, action: {action}\",next_state, reward, done)\n","        state = next_state\n","        if done:\n","            break\n","    vid.close()\n","    env.close()"]},{"cell_type":"markdown","id":"6360c984","metadata":{},"source":["Visualize the policies here.\n","Look at the example to see how to use the provided helper code."]},{"cell_type":"code","execution_count":null,"id":"b19557b0-9fb4-4d0f-af14-6a90dda1245e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2060,"status":"ok","timestamp":1717936246174,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"b19557b0-9fb4-4d0f-af14-6a90dda1245e","outputId":"43184507-4c71-4648-b3a0-c18fd971e989","tags":[]},"outputs":[],"source":["env_name = \"CartPole-v1\"\n","\n","show_video_of_model(env_name, env_1d, angle_policy)\n","show_video(env_name)"]},{"cell_type":"markdown","id":"d11d6e57-b055-45e5-83c9-99ac3d39cde4","metadata":{"id":"d11d6e57-b055-45e5-83c9-99ac3d39cde4"},"source":["## 2D - Frozen Lake"]},{"cell_type":"markdown","id":"6be61d3f-a188-4377-b944-a9b4ee06a3d1","metadata":{"id":"6be61d3f-a188-4377-b944-a9b4ee06a3d1"},"source":["Frozen lake is a toy text environment involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. <br>\n","\n","We can also set the lake to be slippery so that the agent does not always move in the intended direction. \\but here, we will only look at the non-slippery case. But you are welcome to try the slippery one.<br>\n","\n","You can read more about the environment [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n","\n","![Frozen Lake](https://gymnasium.farama.org/_images/frozen_lake.gif)\n"]},{"cell_type":"markdown","id":"1221d41e-7952-431f-b0b5-54a2f05b3bef","metadata":{"id":"1221d41e-7952-431f-b0b5-54a2f05b3bef"},"source":["|   |   |\n","|---|---|\n","| Action Space | Discrete(4) |\n","| Observation Space | Discrete(16) |\n","| Import | `gym.make(\"FrozenLake-v1\")` |\n","\n","\n","Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H)\n","by walking over the Frozen(F) lake.\n","The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n","\n","\n","### Action Space\n","The agent takes a 1-element vector for actions.\n","The action space is `(dir)`, where `dir` decides direction to move in which can be:\n","\n","- 0: LEFT\n","- 1: DOWN\n","- 2: RIGHT\n","- 3: UP\n","\n","### Observation Space\n","The observation is a value representing the agent's current position as\n","current_row * nrows + current_col (where both the row and col start at 0).\n","For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n","The number of possible observations is dependent on the size of the map.\n","For example, the 4x4 map has 16 possible observations.\n","\n","### Rewards\n","\n","Reward schedule:\n","- Reach goal(G): +1\n","- Reach hole(H): 0\n","- Reach frozen(F): 0\n","\n","### Arguments\n","\n","```\n","gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n","```\n","\n","`desc`: Used to specify custom map for frozen lake. For example,\n","\n","    desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"].\n","\n","    A random generated map can be specified by calling the function `generate_random_map`. For example,\n","\n","    ```\n","    from gym.envs.toy_text.frozen_lake import generate_random_map\n","\n","    gym.make('FrozenLake-v1', desc=generate_random_map(size=8))\n","    ```\n","\n","`map_name`: ID to use any of the preloaded maps.\n","\n","    \"4x4\":[\n","        \"SFFF\",\n","        \"FHFH\",\n","        \"FFFH\",\n","        \"HFFG\"\n","        ]\n","\n","    \"8x8\": [\n","        \"SFFFFFFF\",\n","        \"FFFFFFFF\",\n","        \"FFFHFFFF\",\n","        \"FFFFFHFF\",\n","        \"FFFHFFFF\",\n","        \"FHHFFFHF\",\n","        \"FHFFHFHF\",\n","        \"FFFHFFFG\",\n","    ]\n","\n","`is_slippery`: True/False. If True will move in intended direction with\n","probability of 1/3 else will move in either perpendicular direction with\n","equal probability of 1/3 in both directions.\n","\n","    For example, if action is left and is_slippery is True, then:\n","    - P(move left)=1/3\n","    - P(move up)=1/3\n","    - P(move down)=1/3\n"]},{"cell_type":"markdown","id":"81b34434","metadata":{},"source":["Create the FrozenLake Environment"]},{"cell_type":"code","execution_count":null,"id":"1039715f-b1e4-4e9f-b90e-1ed8207a2ef5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1717936268794,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"1039715f-b1e4-4e9f-b90e-1ed8207a2ef5","outputId":"10e8d9c8-671f-4feb-aa33-be776626e5ee","tags":[]},"outputs":[],"source":["env_2d = None"]},{"cell_type":"markdown","id":"9d6e6329","metadata":{},"source":["Implement the following policy functions.\n","\n","Feel free to implement any extra policy if you want."]},{"cell_type":"code","execution_count":33,"id":"159d9ffa-ba01-4deb-8fd9-8a47dcb6f21b","metadata":{"executionInfo":{"elapsed":431,"status":"ok","timestamp":1717936294495,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"159d9ffa-ba01-4deb-8fd9-8a47dcb6f21b","tags":[]},"outputs":[],"source":["# Define the \"Always Left\" policy\n","def always_left_policy(observation):\n","    pass\n","\n","# Define the \"Always Right\" policy\n","def always_right_policy(observation):\n","    pass\n","\n","# Define a random policy\n","def random_policy(observation):\n","    pass\n","\n","# Define a custom algorithm(one you think will work) which uses\n","# the positions of agent and the goal to reach the goal\n","def position_based_policy(observation):\n","    pass"]},{"cell_type":"markdown","id":"9f96d244","metadata":{},"source":["### FrozenLake policy evaluations\n","\n","evaluate the policies you've made above"]},{"cell_type":"code","execution_count":null,"id":"14babddf","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0984621a-97e6-477e-9c88-9b10c9795a3c","metadata":{"id":"0984621a-97e6-477e-9c88-9b10c9795a3c"},"source":["### Visualization\n","\n","Visualize the custom position policy you made, as well as any other you want\n","\n","clearly label the visualiztions with the policies"]},{"cell_type":"code","execution_count":null,"id":"885a33f0-8b1f-44d7-855b-ecf335ce03ae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":455,"status":"ok","timestamp":1690281633262,"user":{"displayName":"Naeemullah Khan","userId":"18307185785900481812"},"user_tz":-60},"id":"885a33f0-8b1f-44d7-855b-ecf335ce03ae","jupyter":{"outputs_hidden":true},"outputId":"8a3579d6-76f2-41f2-e683-936a1a060665","tags":[]},"outputs":[],"source":["# Random policy\n","\n","env_name = \"lake-random\"\n","\n","show_video_of_model(env_name, env_2d, random_policy)\n","show_video(env_name)"]},{"cell_type":"code","execution_count":null,"id":"3cc251a7-1313-4ef0-9618-da27bb91b168","metadata":{"id":"3cc251a7-1313-4ef0-9618-da27bb91b168"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Q8vS_pWHkhEhiOLcFgaJvU9Pf8ybLQLk","timestamp":1690271410416}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
