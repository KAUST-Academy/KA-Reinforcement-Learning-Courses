{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNWEGss4RyG1"
      },
      "source": [
        "# Contents\n",
        "\n",
        "In this notebook, you will implement a simple simulation of Multi-Armed Bandit(Explained below) and a simple Q-table based agent to demonstrate exploration vs exploitation and how changing the epsilon affects the learning.\n",
        "\n",
        "\n",
        "## Context: Bandits\n",
        "\n",
        "![Bandit Image](https://gibberblot.github.io/rl-notes/_images/multi-armed-bandit.png)\n",
        "\n",
        "Imagine that you have N number of slot machines (or poker machines in Australia), which are sometimes called one-armed bandits, due to the “arm” on the side that people pull to run again. Over time, each bandit pays a random reward from an unknown probability distribution. Some bandits are morely likely to get a winning payoff than others – we just do not know which ones at the start. The goal is to maximize the total rewards of a sequence of lever pulls of the machine.\n",
        "\n",
        "## Multi-armed bandit problem\n",
        "\n",
        "A multi-armed bandit (also known as an\n",
        "N-armed bandit) is defined by a set of random variables\n",
        " where:\n",
        "\n",
        "$1 \\leq i \\leq N$, such that\n",
        " is the arm of the bandit; and\n",
        "\n",
        " $k$ the index of the play of arm\n",
        ";\n",
        "\n",
        "Successive plays are assumed to be independently distributed, but we do not know the probability distributions of the random variables.\n",
        "\n",
        "The idea is that a gambler iteratively plays rounds, observing the reward from the arm after each round, and can adjust their strategy each time. The aim is to maximise the sum of the rewards collected over all rounds.\n",
        "\n",
        "Multi-arm bandit strategies aim to learn a policy $\\pi(k)$, where $k$\n",
        " is the play.\n",
        "\n",
        "# Tasks\n",
        "\n",
        "You are required to:\n",
        "\n",
        "- Implement the simulation of multi-armed bandit machines.\n",
        "- Implement a simple Q-table based agent.\n",
        "- Implement the epsilon greedy strategy of exploration.\n",
        "- Train your agent over the simulations over multiple episodes, recording the rewards of each step of each episode.\n",
        "- Repeat this process over multiple values of epsilon, spreading out over the range of [0, 1] (atleast 5 different runs)\n",
        "- Visualize the affect that changing epsilon has over the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaUm-s_rRxc5"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing the agent, simulation and exploration-strategy\n",
        "\n",
        "Implement all the functions marked \"required\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fl5vGqbRRxc-"
      },
      "outputs": [],
      "source": [
        "# Simple class to represent the QTable\n",
        "\n",
        "class QTable():\n",
        "    def __init__(self, alpha=0.1, default_q_value=0.0):\n",
        "        # required\n",
        "        pass\n",
        "\n",
        "    def update(self, state, action, delta):\n",
        "        # required\n",
        "        pass\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        # required\n",
        "        pass\n",
        "\n",
        "    def get_best_action(self, state, actions):\n",
        "        # returns the best action available for the current state from the provided actions.\n",
        "        # required\n",
        "        pass\n",
        "\n",
        "\n",
        "# Base class for bandit with our strategy. We will extend this to create a class representing our strategy\n",
        "class MultiArmedBandit():\n",
        "\n",
        "    \"\"\" Select an action for this state given from a list given a Q-function \"\"\"\n",
        "    def select(self, state, actions, qfunction):\n",
        "        # abstract function. do not modify\n",
        "        pass\n",
        "\n",
        "    \"\"\" Reset a multi-armed bandit to its initial configuration \"\"\"\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "\n",
        "\n",
        "# Bandit class which represents the epsilon-greedy approach\n",
        "class EpsilonGreedy(MultiArmedBandit):\n",
        "    def __init__(self, epsilon=0.1):\n",
        "        # required\n",
        "        pass\n",
        "\n",
        "    def select(self, state, actions, qfunction):\n",
        "        # Select a random action with epsilon probability\n",
        "        # required\n",
        "        pass\n",
        "\n",
        "\n",
        "\"\"\" Run a bandit algorithm for a number of episodes, with each episode\n",
        "being a set length.\n",
        "\"\"\"\n",
        "def run_bandit(bandit, episodes=200, episode_length=500, drift=True):\n",
        "\n",
        "    # required\n",
        "\n",
        "    # Implement the function using the following details:\n",
        "\n",
        "    # 1. Assume we have 5 different machines(or arms) each with it's own probability of receiving a reward.\n",
        "    #    Which means there are 5 actions(0, 1, 2, 3, 4) and 5 probabilities(Choose them yourself but dont make them all very high or low).\n",
        "    # 2. In every episode, reset the bandit and keep track of the number of times each action has been selected.\n",
        "    #    Calculate delta as: (reward / times_selected) - (qtable.get_q_value(state, action) / times_selected) where times_selected\n",
        "    #    is the number of times the chosen action has been selected.\n",
        "    # 3. The function needs to return a list(number of episodes) of lists(length of episodes) of rewards\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUqQ8XCxd-ql"
      },
      "source": [
        "## Running the simulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOQawVo7RxdA"
      },
      "outputs": [],
      "source": [
        "epsilons = []  # Choose a list of epsilon values to test. At least 5, spread out over the range of 0 to 1\n",
        "\n",
        "# Run the simulation for each epsilon value and store the reward values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2_BmtSPeD__"
      },
      "source": [
        "## Visualizing the results\n",
        "\n",
        "write code to draw graph to compare the learning of each simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPcXepUMRxdB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kaustenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
