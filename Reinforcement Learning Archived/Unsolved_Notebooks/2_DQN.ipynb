{"cells":[{"cell_type":"markdown","metadata":{"id":"4F0itjdKve8n"},"source":["# Contents: Deep Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"PBgbMXUyvhy4"},"source":["In this notebook, you will implement Deep Q-Learning Reinforcement learning algorithm for Cart Pole Environment.\n","\n","Write the code to define and train the agent.\n","Make sure to include a visualization of the end result in form of a video."]},{"cell_type":"markdown","metadata":{"id":"5HacVpXyvw85"},"source":["## Cartpole"]},{"cell_type":"markdown","metadata":{"id":"xbEBbSSQv3FF"},"source":["As the image below shows, the goal of the agent is to balance a verticle rod on the top of the car. This position is unstable and that is the main reason for the difficulty.\n","\n","![Cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n","\n","The problem is solved if the average of the agent's scores is greater than 195 gathered in 100 episodes.\n","The agent receives reward 1 in each timestep until the psoition of the rod is correct (not inclined too far away from the vertical position).\n","\n","The episode ends if any one of the following occurs:\n","- Pole Angle is greater than ±12°\n","- Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n","- Episode length is greater than 500 (200 for v0)\n","\n","The state is low dimensional and cosists of:\n","* position\n","* velocity\n","* angle\n","* angular velocity\n","\n","You can read more the cartpole environment [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/)"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Q-Learning"]},{"cell_type":"markdown","metadata":{},"source":["The main idea behind Q-learning is that if we had a function\n","$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n","us what our return would be, if we were to take an action in a given\n","state, then we could easily construct a policy that maximizes our\n","rewards:\n","\n","\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n","\n","But this is not scalable. Must compute $Q(s,a)$ for every state-action pair. If state is e.g. current game state pixels, computationally infeasible to compute for entire state space! But, since neural networks are universal function\n","approximators, we can simply create one and train it to resemble\n","$Q^*$.\n","\n","For our training update rule, we'll use a fact that every $Q$\n","function for some policy obeys the Bellman equation:\n","\n","\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n","\n","The difference between the two sides of the equality is known as the\n","temporal difference error, $\\delta$:\n","\n","\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n","\n","To minimise this error, we will use the `Huber\n","loss <https://en.wikipedia.org/wiki/Huber_loss>`__. The Huber loss acts\n","like the mean squared error when the error is small, but like the mean\n","absolute error when the error is large - this makes it more robust to\n","outliers when the estimates of $Q$ are very noisy. We calculate\n","this over a batch of transitions, $B$, sampled from the replay\n","memory:\n","\n","\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n","\n","\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n","     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n","     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n","   \\end{cases}\\end{align}\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["![dqn_target.png](dqn_target.png)"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PTAvIn9tRfP"},"outputs":[],"source":["import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1687478275564,"user":{"displayName":"Muhammad Mubashar","userId":"14515435323579848862"},"user_tz":-60},"id":"UozhBzRv5wFG","outputId":"09cb7a4d-0c12-4ce0-d1cf-a95b34f14405"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687478275564,"user":{"displayName":"Muhammad Mubashar","userId":"14515435323579848862"},"user_tz":-60},"id":"70UaXBCE_zvV","outputId":"f6bea99c-c3d9-4f1b-843d-3670e76500e9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["# Create the environment\n","env = gym.make(\"CartPole-v1\")"]},{"cell_type":"markdown","metadata":{"id":"DBiJ6RqCDcYR"},"source":["### Solve here\n","\n","write the code to define and train the agent:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4oDvqNXGE3-D"},"source":["### Visualization\n","\n","You are provided with some functions which will help you visualize the results as a video.\n","Feel free to wrie your own code for visualization if you prefer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT7EFCqfEAYH"},"outputs":[],"source":["# For visualization\n","from gym.wrappers.monitoring import video_recorder\n","from IPython.display import HTML\n","from IPython import display\n","import glob\n","import base64, io, os\n","\n","os.environ['SDL_VIDEODRIVER']='dummy'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmyN1qwqFAcF"},"outputs":[],"source":["os.makedirs(\"video\", exist_ok=True)\n","\n","def show_video(env_name):\n","    # Function to show a video in the notebook. Do not modify.\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = 'video/{}.mp4'.format(env_name)\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","def show_video_of_model(env_name, max_steps=200):\n","    vid = video_recorder.VideoRecorder(env, path=\"video/{}.mp4\".format(env_name))\n","    state = env.reset()\n","    done = False\n","    for t in range(max_steps):\n","        vid.capture_frame()\n","\n","        # Write your code to choose an action here.\n","        action = 0\n","\n","\n","        next_state, reward, done, info = env.step(action.item())\n","        next_state = torch.tensor([next_state], device=device)\n","        print(f\"state: {state}, action: {action}\",next_state, reward, done)\n","        state = next_state\n","        if done:\n","            break\n","    vid.close()\n","    env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1522,"status":"ok","timestamp":1687478954996,"user":{"displayName":"Muhammad Mubashar","userId":"14515435323579848862"},"user_tz":-60},"id":"EDnpeR1aFZNW","outputId":"33a14907-cfdd-442a-a15e-714964343eb1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]}],"source":["show_video_of_model(\"CartPole-v1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":476},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687478954996,"user":{"displayName":"Muhammad Mubashar","userId":"14515435323579848862"},"user_tz":-60},"id":"i1lgvG_TFdf3","outputId":"89c7ea54-7bf6-45aa-fa7c-85d7b19ea715"},"outputs":[],"source":["show_video(\"CartPole-v1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9DsxV6PFgOO"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN4B/DAQ7Eo9B/4LJuBFNl/","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
